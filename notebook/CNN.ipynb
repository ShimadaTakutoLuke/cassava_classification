{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データセットのサイズ: 14545\n",
      "検証データセットのサイズ: 3637\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# データの前処理設定\n",
    "transfrom = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5))\n",
    "])\n",
    "\n",
    "# データセットの読み込み\n",
    "data_dir = \"/home/dataset/leaf_dataset/train\"\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transfrom)\n",
    "\n",
    "# 8:2でデータセットを分割\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# データローダー設定\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(f\"訓練データセットのサイズ: {len(train_dataset)}\")\n",
    "print(f\"検証データセットのサイズ: {len(val_dataset)}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "モデルの構造\n",
      "CNNModel(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=2048, out_features=100, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=5, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CNNモデルの定義（先ほどCNNModelクラスをそのまま使用）\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 100)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "        self.flatten = nn.Flatten()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# クラス数をデータセットに基づいて設定\n",
    "num_classes = len(dataset.classes)  \n",
    "model = CNNModel(num_classes=num_classes)\n",
    "\n",
    "print(\"モデルの構造\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 1.1782, Accuracy: 61.55%, Val Loss: 1.1000, Val Accuracy: 62.85%\n",
      "Epoch [2], Loss: 1.1399, Accuracy: 61.66%, Val Loss: 1.1170, Val Accuracy: 62.85%\n",
      "Epoch [3], Loss: 1.1168, Accuracy: 61.64%, Val Loss: 1.0866, Val Accuracy: 62.88%\n",
      "Epoch [4], Loss: 1.1049, Accuracy: 61.87%, Val Loss: 1.0715, Val Accuracy: 63.32%\n",
      "Epoch [5], Loss: 1.0884, Accuracy: 62.05%, Val Loss: 1.0560, Val Accuracy: 63.76%\n",
      "Epoch [6], Loss: 1.0739, Accuracy: 62.48%, Val Loss: 1.0602, Val Accuracy: 63.87%\n",
      "Epoch [7], Loss: 1.0578, Accuracy: 62.72%, Val Loss: 1.0613, Val Accuracy: 63.79%\n",
      "Epoch [8], Loss: 1.0447, Accuracy: 62.95%, Val Loss: 1.0366, Val Accuracy: 64.39%\n",
      "Epoch [9], Loss: 1.0352, Accuracy: 63.11%, Val Loss: 1.0445, Val Accuracy: 64.48%\n",
      "Epoch [10], Loss: 1.0195, Accuracy: 63.36%, Val Loss: 1.0432, Val Accuracy: 64.48%\n",
      "Epoch [11], Loss: 1.0025, Accuracy: 63.93%, Val Loss: 1.0435, Val Accuracy: 64.56%\n",
      "Epoch [12], Loss: 0.9838, Accuracy: 64.43%, Val Loss: 1.0489, Val Accuracy: 64.56%\n",
      "Epoch [13], Loss: 0.9620, Accuracy: 64.74%, Val Loss: 1.0628, Val Accuracy: 63.93%\n",
      "Epoch [14], Loss: 0.9416, Accuracy: 65.44%, Val Loss: 1.0759, Val Accuracy: 64.20%\n",
      "Epoch [15], Loss: 0.9218, Accuracy: 65.75%, Val Loss: 1.0807, Val Accuracy: 63.93%\n",
      "Epoch [16], Loss: 0.8974, Accuracy: 66.59%, Val Loss: 1.1044, Val Accuracy: 64.20%\n",
      "Epoch [17], Loss: 0.8802, Accuracy: 66.69%, Val Loss: 1.1021, Val Accuracy: 63.05%\n",
      "Epoch [18], Loss: 0.8580, Accuracy: 67.62%, Val Loss: 1.1173, Val Accuracy: 64.06%\n",
      "Epoch [19], Loss: 0.8406, Accuracy: 67.84%, Val Loss: 1.1401, Val Accuracy: 64.28%\n",
      "Epoch [20], Loss: 0.8149, Accuracy: 68.47%, Val Loss: 1.1614, Val Accuracy: 63.84%\n",
      "Epoch [21], Loss: 0.7934, Accuracy: 69.26%, Val Loss: 1.1713, Val Accuracy: 63.46%\n",
      "Epoch [22], Loss: 0.7690, Accuracy: 70.04%, Val Loss: 1.2687, Val Accuracy: 63.71%\n",
      "Epoch [23], Loss: 0.7544, Accuracy: 70.31%, Val Loss: 1.2491, Val Accuracy: 62.11%\n",
      "Epoch [24], Loss: 0.7228, Accuracy: 71.54%, Val Loss: 1.3301, Val Accuracy: 62.55%\n",
      "Epoch [25], Loss: 0.7064, Accuracy: 71.69%, Val Loss: 1.3042, Val Accuracy: 62.39%\n",
      "Epoch [26], Loss: 0.6781, Accuracy: 72.92%, Val Loss: 1.3489, Val Accuracy: 61.78%\n",
      "Epoch [27], Loss: 0.6599, Accuracy: 73.91%, Val Loss: 1.3773, Val Accuracy: 60.79%\n",
      "Epoch [28], Loss: 0.6381, Accuracy: 74.60%, Val Loss: 1.5131, Val Accuracy: 61.70%\n",
      "Epoch [29], Loss: 0.6279, Accuracy: 74.64%, Val Loss: 1.4505, Val Accuracy: 59.39%\n",
      "Epoch [30], Loss: 0.5975, Accuracy: 75.34%, Val Loss: 1.5253, Val Accuracy: 59.64%\n",
      "Epoch [31], Loss: 0.5888, Accuracy: 76.08%, Val Loss: 1.5642, Val Accuracy: 61.23%\n",
      "Epoch [32], Loss: 0.5648, Accuracy: 76.56%, Val Loss: 1.6569, Val Accuracy: 61.07%\n",
      "Epoch [33], Loss: 0.5525, Accuracy: 77.50%, Val Loss: 1.6843, Val Accuracy: 60.49%\n",
      "Epoch [34], Loss: 0.5407, Accuracy: 77.57%, Val Loss: 1.7952, Val Accuracy: 60.13%\n",
      "Epoch [35], Loss: 0.5286, Accuracy: 78.16%, Val Loss: 1.8146, Val Accuracy: 60.35%\n",
      "Epoch [36], Loss: 0.5146, Accuracy: 78.71%, Val Loss: 1.7839, Val Accuracy: 58.32%\n",
      "Epoch [37], Loss: 0.5016, Accuracy: 78.90%, Val Loss: 1.8442, Val Accuracy: 57.93%\n",
      "Epoch [38], Loss: 0.4895, Accuracy: 79.20%, Val Loss: 2.0714, Val Accuracy: 60.13%\n",
      "Epoch [39], Loss: 0.4826, Accuracy: 79.89%, Val Loss: 1.9530, Val Accuracy: 57.60%\n",
      "Epoch [40], Loss: 0.4713, Accuracy: 80.14%, Val Loss: 2.1185, Val Accuracy: 59.75%\n",
      "Epoch [41], Loss: 0.4520, Accuracy: 81.14%, Val Loss: 2.1043, Val Accuracy: 59.03%\n",
      "Epoch [42], Loss: 0.4464, Accuracy: 81.22%, Val Loss: 2.2865, Val Accuracy: 59.83%\n",
      "Epoch [43], Loss: 0.4366, Accuracy: 81.44%, Val Loss: 2.3094, Val Accuracy: 58.29%\n",
      "Epoch [44], Loss: 0.4219, Accuracy: 81.84%, Val Loss: 2.4569, Val Accuracy: 59.88%\n",
      "Epoch [45], Loss: 0.4300, Accuracy: 81.82%, Val Loss: 2.4982, Val Accuracy: 60.32%\n",
      "Epoch [46], Loss: 0.4216, Accuracy: 81.99%, Val Loss: 2.4231, Val Accuracy: 59.11%\n",
      "Epoch [47], Loss: 0.4121, Accuracy: 82.45%, Val Loss: 2.5843, Val Accuracy: 58.89%\n",
      "Epoch [48], Loss: 0.4054, Accuracy: 82.56%, Val Loss: 2.3647, Val Accuracy: 57.03%\n",
      "Epoch [49], Loss: 0.3898, Accuracy: 83.29%, Val Loss: 2.7353, Val Accuracy: 58.95%\n",
      "Epoch [50], Loss: 0.3860, Accuracy: 83.53%, Val Loss: 2.8020, Val Accuracy: 59.58%\n",
      "Epoch [51], Loss: 0.3827, Accuracy: 83.89%, Val Loss: 2.7537, Val Accuracy: 58.67%\n",
      "Epoch [52], Loss: 0.3699, Accuracy: 83.97%, Val Loss: 2.8810, Val Accuracy: 57.99%\n",
      "Epoch [53], Loss: 0.3743, Accuracy: 84.35%, Val Loss: 2.7474, Val Accuracy: 57.90%\n",
      "Epoch [54], Loss: 0.3690, Accuracy: 84.32%, Val Loss: 3.0546, Val Accuracy: 59.72%\n",
      "Epoch [55], Loss: 0.3589, Accuracy: 84.50%, Val Loss: 3.1955, Val Accuracy: 59.97%\n",
      "Epoch [56], Loss: 0.3620, Accuracy: 84.51%, Val Loss: 3.1312, Val Accuracy: 58.67%\n",
      "Epoch [57], Loss: 0.3503, Accuracy: 85.12%, Val Loss: 3.0082, Val Accuracy: 58.45%\n",
      "Epoch [58], Loss: 0.3505, Accuracy: 84.94%, Val Loss: 3.2528, Val Accuracy: 58.23%\n",
      "Epoch [59], Loss: 0.3502, Accuracy: 84.81%, Val Loss: 3.1363, Val Accuracy: 57.66%\n",
      "Epoch [60], Loss: 0.3455, Accuracy: 85.29%, Val Loss: 3.1388, Val Accuracy: 58.70%\n",
      "Epoch [61], Loss: 0.3403, Accuracy: 85.62%, Val Loss: 3.1984, Val Accuracy: 58.37%\n",
      "Epoch [62], Loss: 0.3362, Accuracy: 85.70%, Val Loss: 3.4634, Val Accuracy: 58.89%\n",
      "Epoch [63], Loss: 0.3299, Accuracy: 85.96%, Val Loss: 3.4521, Val Accuracy: 59.42%\n",
      "Epoch [64], Loss: 0.3263, Accuracy: 86.10%, Val Loss: 3.5675, Val Accuracy: 58.67%\n",
      "Epoch [65], Loss: 0.3328, Accuracy: 85.70%, Val Loss: 3.4771, Val Accuracy: 58.18%\n",
      "Epoch [66], Loss: 0.3189, Accuracy: 86.13%, Val Loss: 3.5902, Val Accuracy: 58.65%\n",
      "Epoch [67], Loss: 0.3294, Accuracy: 85.88%, Val Loss: 3.4997, Val Accuracy: 57.60%\n",
      "Epoch [68], Loss: 0.3159, Accuracy: 86.46%, Val Loss: 3.7685, Val Accuracy: 59.55%\n",
      "Epoch [69], Loss: 0.3143, Accuracy: 86.37%, Val Loss: 3.8876, Val Accuracy: 59.20%\n",
      "Epoch [70], Loss: 0.3195, Accuracy: 86.63%, Val Loss: 4.0127, Val Accuracy: 59.86%\n",
      "Epoch [71], Loss: 0.3141, Accuracy: 86.69%, Val Loss: 3.7862, Val Accuracy: 59.03%\n",
      "Epoch [72], Loss: 0.3126, Accuracy: 86.64%, Val Loss: 3.8310, Val Accuracy: 58.65%\n",
      "Epoch [73], Loss: 0.3108, Accuracy: 86.71%, Val Loss: 3.8486, Val Accuracy: 58.95%\n",
      "Epoch [74], Loss: 0.3203, Accuracy: 86.09%, Val Loss: 3.8331, Val Accuracy: 60.24%\n",
      "Epoch [75], Loss: 0.3071, Accuracy: 86.95%, Val Loss: 3.8988, Val Accuracy: 59.11%\n",
      "Epoch [76], Loss: 0.2961, Accuracy: 87.47%, Val Loss: 4.2745, Val Accuracy: 59.91%\n",
      "Epoch [77], Loss: 0.3077, Accuracy: 86.94%, Val Loss: 4.4050, Val Accuracy: 59.83%\n",
      "Epoch [78], Loss: 0.2982, Accuracy: 87.33%, Val Loss: 4.2840, Val Accuracy: 59.44%\n",
      "Epoch [79], Loss: 0.2950, Accuracy: 87.24%, Val Loss: 4.0121, Val Accuracy: 57.85%\n",
      "Epoch [80], Loss: 0.2916, Accuracy: 87.57%, Val Loss: 4.2914, Val Accuracy: 59.25%\n",
      "Epoch [81], Loss: 0.2953, Accuracy: 87.47%, Val Loss: 4.0327, Val Accuracy: 58.92%\n",
      "Epoch [82], Loss: 0.2841, Accuracy: 88.07%, Val Loss: 4.6818, Val Accuracy: 59.03%\n",
      "Epoch [83], Loss: 0.2854, Accuracy: 87.89%, Val Loss: 4.5648, Val Accuracy: 59.80%\n",
      "Epoch [84], Loss: 0.2862, Accuracy: 88.07%, Val Loss: 4.5484, Val Accuracy: 58.95%\n",
      "Epoch [85], Loss: 0.2871, Accuracy: 87.82%, Val Loss: 4.0656, Val Accuracy: 57.19%\n",
      "Epoch [86], Loss: 0.2877, Accuracy: 87.91%, Val Loss: 4.4897, Val Accuracy: 58.89%\n",
      "Epoch [87], Loss: 0.2777, Accuracy: 88.31%, Val Loss: 4.5751, Val Accuracy: 58.23%\n",
      "Epoch [88], Loss: 0.2829, Accuracy: 88.06%, Val Loss: 4.5449, Val Accuracy: 58.70%\n",
      "Epoch [89], Loss: 0.2846, Accuracy: 88.10%, Val Loss: 4.6359, Val Accuracy: 59.03%\n",
      "Epoch [90], Loss: 0.2876, Accuracy: 87.97%, Val Loss: 4.7472, Val Accuracy: 59.39%\n",
      "Epoch [91], Loss: 0.2859, Accuracy: 87.85%, Val Loss: 4.6326, Val Accuracy: 58.89%\n",
      "Epoch [92], Loss: 0.2739, Accuracy: 88.29%, Val Loss: 4.9746, Val Accuracy: 59.61%\n",
      "Epoch [93], Loss: 0.2747, Accuracy: 88.23%, Val Loss: 4.8979, Val Accuracy: 59.53%\n",
      "Epoch [94], Loss: 0.2725, Accuracy: 88.50%, Val Loss: 5.2100, Val Accuracy: 59.69%\n",
      "Epoch [95], Loss: 0.2724, Accuracy: 88.78%, Val Loss: 5.0341, Val Accuracy: 58.84%\n",
      "Epoch [96], Loss: 0.2739, Accuracy: 88.31%, Val Loss: 4.9183, Val Accuracy: 59.00%\n",
      "Epoch [97], Loss: 0.2632, Accuracy: 88.93%, Val Loss: 5.0442, Val Accuracy: 58.95%\n",
      "Epoch [98], Loss: 0.2672, Accuracy: 88.99%, Val Loss: 4.9020, Val Accuracy: 57.93%\n",
      "Epoch [99], Loss: 0.2678, Accuracy: 88.79%, Val Loss: 4.9460, Val Accuracy: 58.37%\n",
      "Epoch [100], Loss: 0.2674, Accuracy: 88.61%, Val Loss: 4.7745, Val Accuracy: 58.32%\n",
      "Epoch [1], Loss: 0.2639, Accuracy: 88.91%, Val Loss: 5.4070, Val Accuracy: 59.83%\n",
      "Epoch [2], Loss: 0.2598, Accuracy: 89.43%, Val Loss: 5.3165, Val Accuracy: 58.32%\n",
      "Epoch [3], Loss: 0.2747, Accuracy: 88.55%, Val Loss: 5.2229, Val Accuracy: 59.53%\n",
      "Epoch [4], Loss: 0.2640, Accuracy: 88.96%, Val Loss: 4.8354, Val Accuracy: 57.38%\n",
      "Epoch [5], Loss: 0.2620, Accuracy: 89.07%, Val Loss: 5.4562, Val Accuracy: 58.81%\n",
      "Epoch [6], Loss: 0.2666, Accuracy: 88.80%, Val Loss: 4.7975, Val Accuracy: 57.79%\n",
      "Epoch [7], Loss: 0.2576, Accuracy: 89.22%, Val Loss: 5.5348, Val Accuracy: 59.06%\n",
      "Epoch [8], Loss: 0.2563, Accuracy: 89.24%, Val Loss: 5.4649, Val Accuracy: 59.47%\n",
      "Epoch [9], Loss: 0.2531, Accuracy: 89.40%, Val Loss: 5.3616, Val Accuracy: 58.65%\n",
      "Epoch [10], Loss: 0.2656, Accuracy: 88.92%, Val Loss: 5.0250, Val Accuracy: 58.54%\n",
      "Epoch [11], Loss: 0.2582, Accuracy: 89.38%, Val Loss: 5.4387, Val Accuracy: 59.44%\n",
      "Epoch [12], Loss: 0.2555, Accuracy: 89.12%, Val Loss: 5.0676, Val Accuracy: 58.10%\n",
      "Epoch [13], Loss: 0.2557, Accuracy: 89.37%, Val Loss: 5.0663, Val Accuracy: 57.71%\n",
      "Epoch [14], Loss: 0.2539, Accuracy: 89.38%, Val Loss: 5.1177, Val Accuracy: 57.41%\n",
      "Epoch [15], Loss: 0.2508, Accuracy: 89.29%, Val Loss: 5.9314, Val Accuracy: 60.10%\n",
      "Epoch [16], Loss: 0.2497, Accuracy: 89.47%, Val Loss: 5.4948, Val Accuracy: 58.73%\n",
      "Epoch [17], Loss: 0.2508, Accuracy: 89.60%, Val Loss: 5.7657, Val Accuracy: 60.35%\n",
      "Epoch [18], Loss: 0.2526, Accuracy: 89.26%, Val Loss: 5.6250, Val Accuracy: 59.06%\n",
      "Epoch [19], Loss: 0.2530, Accuracy: 89.70%, Val Loss: 5.4072, Val Accuracy: 57.90%\n",
      "Epoch [20], Loss: 0.2544, Accuracy: 89.61%, Val Loss: 5.9939, Val Accuracy: 60.30%\n",
      "Epoch [21], Loss: 0.2428, Accuracy: 89.74%, Val Loss: 5.4931, Val Accuracy: 58.92%\n",
      "Epoch [22], Loss: 0.2474, Accuracy: 89.81%, Val Loss: 5.6515, Val Accuracy: 58.51%\n",
      "Epoch [23], Loss: 0.2574, Accuracy: 89.52%, Val Loss: 5.7755, Val Accuracy: 59.64%\n",
      "Epoch [24], Loss: 0.2475, Accuracy: 89.78%, Val Loss: 5.5051, Val Accuracy: 57.88%\n",
      "Epoch [25], Loss: 0.2452, Accuracy: 89.74%, Val Loss: 5.8617, Val Accuracy: 59.61%\n",
      "Epoch [26], Loss: 0.2516, Accuracy: 89.58%, Val Loss: 6.1835, Val Accuracy: 60.30%\n",
      "Epoch [27], Loss: 0.2483, Accuracy: 89.82%, Val Loss: 6.1376, Val Accuracy: 59.91%\n",
      "Epoch [28], Loss: 0.2413, Accuracy: 90.03%, Val Loss: 5.7567, Val Accuracy: 59.00%\n",
      "Epoch [29], Loss: 0.2455, Accuracy: 89.84%, Val Loss: 5.9297, Val Accuracy: 59.14%\n",
      "Epoch [30], Loss: 0.2356, Accuracy: 90.36%, Val Loss: 6.0124, Val Accuracy: 59.09%\n",
      "Epoch [31], Loss: 0.2398, Accuracy: 89.91%, Val Loss: 6.0338, Val Accuracy: 59.80%\n",
      "Epoch [32], Loss: 0.2443, Accuracy: 89.88%, Val Loss: 6.0427, Val Accuracy: 59.42%\n",
      "Epoch [33], Loss: 0.2347, Accuracy: 90.20%, Val Loss: 5.7443, Val Accuracy: 59.80%\n",
      "Epoch [34], Loss: 0.2418, Accuracy: 89.94%, Val Loss: 6.2794, Val Accuracy: 59.80%\n",
      "Epoch [35], Loss: 0.2303, Accuracy: 90.39%, Val Loss: 6.2526, Val Accuracy: 59.69%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 178\u001b[0m\n\u001b[1;32m    175\u001b[0m     writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# モデルの訓練\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m    112\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    113\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 115\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 勾配の初期化\u001b[39;49;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1327\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1293\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1293\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1295\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1131\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/multiprocessing/connection.py:1136\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# GPUが使えるか確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 損失関数と最適化関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TensorBoardのWriterを定義\n",
    "writer = SummaryWriter(log_dir=\"runs/experiment1\")\n",
    "\n",
    "# トレーニング関数\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # トレーニングフェーズ\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 勾配の初期化\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 順伝播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 逆伝播と最適化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # ロスの累計\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # 正解率の計算\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "        # TrainデータにおけるLossとAccuracy\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "\n",
    "        # 検証フェーズ\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                # 順伝播\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # 検証ロスの累計\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # 検証データでの正解率の計算\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # ValidationデータにおけるLossとAccuracy\n",
    "        val_epoch_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "\n",
    "        # TensorBoardにログを追加 (TrainとValidationのLoss/Accuracy)\n",
    "        writer.add_scalar(\"Loss/train\", epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", accuracy, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/val\", val_accuracy, epoch)\n",
    "        \n",
    "        # 結果の出力\n",
    "        print(f\"Epoch [{epoch+1}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%, Val Loss: {val_epoch_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# モデルの訓練\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータに対する正解率: 32.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 評価関数\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # 順伝播のみ\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 正解率の計算\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"テストデータに対する正解率: {accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# データのディレクトリ設定(評価用)\n",
    "test_dir = \"/home/dataset/check\"\n",
    "\n",
    "# 評価データの前処理設定\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5))\n",
    "])\n",
    "\n",
    "# 評価データセットの読み込み\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "\n",
    "# モデルの評価\n",
    "evaluate_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
